%\documentclass{article}
%\usepackage{amsmath}
%\usepackage{a4wide}
%\providecommand{\ve}[1]{\boldsymbol{#1}}
%\newcommand{\thetn}{\ve{\theta}}
%\newcommand{\p}{P_{\thetn}}
%\title{k-mixture Conditional Sampler}
%\begin{document}
%\maketitle

More efficient sampling can be achieved by using a sampling distribution that explicitly considers the observations.  In particular, one can sample conditioned on the \emph{next} observation.  Thus, if $u$ is the time of the previous observation, and $v$ is the time of the next observation, for all $s \in(u,v]$, one would like to define $\q$ as $\p[\ve{H}_s|\ve{H}_{s-1}^{(i)},O_v]$, which may be expanded:

\begin{multline} \label{eq:XO_opt}
%\begin{split}
\p[\ve{H}_s|\ve{H}_{s-1}^{(i)},O_v] = \p [\{\ve{h},n,C\}_s | \{\ve{h},n,C\}_{s-1}^{(i)}, F_v]
\\ = \frac{1}{Z} \p \big[ \ve{h}_s | \ve{h}^{(i)}_{s-1}, n_{s-1}^{(i)} \big] \p \big[ n_s | \ve{h}^{(i)}_{s} \big] \p \big[ C_s | C^{(i)}_{s-1}, n_s^{(i)} \big] \p \big[ F_v | C_s \big]
%\end{split}
\end{multline}

\noindent where the only difference between \eqref{eq:XO_opt} and \eqref{eq:NO_opt} is the presence of the future observation probability,  $\p[F_v | C_s]$.  Thus, this future observation probability must be computed for all $s \in(u,v]$.  The idea for computing this future observation probability is depicted in Figure \ref{fig:pf}\textbf{(B)}.  At time $v$ (the time of the next observation), we have a Gaussian distribution governing where the calcium may be, centered at the observation value (this follows from \eqref{eq:F_t}) (black line at $v$, $+$ marks the mean). 

At $v-1$, the neuron could either have spiked or not.  If the neuron did not spike, for the calcium to be where it is at time $v$, the calcium should do the inverse of decay (remember that the recursion goes backwards).  However, if the neuron did spike, the calcium should be $\beta$ \emph{below} its value at time $v$.  In either case, because the noise on the calcium transitions is Gaussian, the distribution maintains its Gaussianity, and slightly increases its variance.  Thus, at time $v-1$, the distribution of calcium is a \emph{mixture of Gaussians}.  At $v-1$, we have a \emph{2-component mixture}, one component for $n_{v-1}=1$ and one for $n_{v-1}=0$ (black line at $v-1$, $+$'s mark the means for the 2 components). The component coefficient (probability of being in that component), $a_{n,v-1}$ is the expected probability of spiking or not.

Recursing backward one more step yields a 4-component mixture, as each component in the mixture at $v-1$ could have gotten there either from the neuron spiking or not at time $v-2$ (black line and $+$'s at $v-2$).  The coefficient for each of the 4 components is proportional to the expected probability of having that particular \emph{sequence} of spikes, i.e., at $v-2$, we have 4 possible sequences: $(00)$, $(01)$, $(10)$, and $(11)$ corresponding to no spikes, only spiking at time $v-1$, only spiking at time $v-2$ and spiking at both $v-1$ and $v-2$, respectively.  This suggests that at time $v-s$, there will be a $2^{v-s}$-component mixture, where each component may be indexed by a binary number of length $v-s$. 

An interesting observation is that at $v-2$, two of the means seem to be completely overlapping.  In fact, those two components correspond to $(01)$ and $(10)$, i.e., the sequences with exactly one spike.  This follows from the fact that the calcium time constant is much larger than the step size, $\tau_c \gg dt$; therefore, the amount of decay (or rather, inverse decay) in a few time steps is essentially negligible.  One can therefore approximate the two components corresponding to a single spike at $v-2$ into one Gaussian.  More generally, at any time $v-s$, all the components resulting from the same number of spikes between $s$ and $v$ can be combined into a single component.  One must simply take care to modify the component weights, means, and variances appropriately.  Upon doing so, at time $s$, instead of a mixture with $2^{v-s}$ components, we are left with a mixture of $v-s+1$ components (i.e., one component per possible number of spikes until time $v$)

This approximation drastically reduces the computational load of using the conditional sampler.  For instance, imagine that the frame rate, $f$, is $10$ Hz and the step size, $dt$, is $2$ msec.  In that scenario, without the approximation, there would be up to $v-u=1/(f dt)=50$ time steps between observations, yielding a $2^{50}$ component mixture in the no-approximation situation, versus a $51$ component mixture when using our approximation!  As such, we use this approximation for $\p[F_v | C_s]$ when sampling.  

Having $\p[F_v | C_s]$ for all $s\in(u,v]$, one can now sample from each of the hidden states by marginalizing \eqref{eq:XO_opt} with respect to the other hidden states.  For example, to construct to spike sampling distribution, $q[n_s]$, integrate out $\ve{h}_s$ and $C_s$ from \eqref{eq:XO_opt}:

\begin{align}
q[n_s] &\sim \frac{1}{Z} \iint  \p \big[ n_s | \ve{h}_s \big] \p \big[ \ve{h}_s | \ve{h}^{(i)}_{s-1}, n_{s-1}^{(i)} \big] \p \big[ C_s | C^{(i)}_{s-1}, n_s \big] \p[F_v|C_s] d\ve{h}_s dC_s
\end{align}

\noindent for both $n_s=0$ and $n_s=1$. Then, sample $n_s$ from this new Bernoulli distribution, which explicitly considers the observations.  One proceeds similarly to construct $q[\ve{h}_s]$ and $q[C_s]$, and then samples from those as well.  Having these sampling distributions, one must then compute the weights of each particle using

\begin{align} \label{eq:cond_w}
\widetilde{w}_s^{(i)} &= \frac{\p\big[O_s | \ve{H}_s^{(i)}\big]
\p\big[\ve{H}_s^{(i)} | \ve{H}_{s-1}^{(i)}\big] w_{s-1}^{(i)}}{q\big[\ve{h}_s^{(i)}\big] q\big[n_s^{(i)}\big] q\big[C_s^{(i)}\big]}
\end{align}

\noindent Then, at observation times, one resamples if appropriate.  Appendix \ref{sec:cond_samp} provides the mathematical details of how to compute $\p[F_v | C_s]$ and the sampling distributions, and Table \ref{tab:back} provides pseudocode for implementing the conditional particle filter, which can act as the forward step in the SMC-EM algorithm.

%\end{document} 